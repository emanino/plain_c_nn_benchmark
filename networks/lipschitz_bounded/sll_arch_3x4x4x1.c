// This file is computer-generated by onnx2c 
// (TODO: add creating command line here)
// (TODO: print creation date here )

// ONNX model:
// produced by pytorch, version 2.0.1
// ONNX IR version: 14
// Model documentation: 
/*

*/

#include <float.h>
#include <math.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>
#define MAX(X,Y) ( X > Y ? X : Y)
#define MIN(X,Y) ( X < Y ? X : Y)
#define CLIP(X,L) ( MAX(MIN(X,L), -L) )

static const float tensor__0_Constant_8_output_0[1] = 
{0.0000000000000000000f};
static const float tensor__1_Constant_output_0[4][4] = 
{
  {-0.57191801071166992188f, -0.37927693128585815430f, -0.32979178428649902344f, -0.66950309276580810547f},
  {0.33288872241973876953f, 0.043159000575542449951f, 0.39116257429122924805f, -0.38304737210273742676f},
  {0.50928938388824462891f, 0.49622687697410583496f, -0.35915032029151916504f, 0.23497977852821350098f},
  {0.55458402633666992188f, -0.54058611392974853516f, -0.42702928185462951660f, -0.33393284678459167480f}
};
static const float tensor__1_Constant_1_output_0[4] = 
{0.47967907786369323730f, 0.54041445255279541016f, 0.73867255449295043945f, -0.17797181010246276855f};
static const float tensor__1_Constant_2_output_0[4][4] = 
{
  {-0.44447538256645202637f, -0.29476124048233032227f, -0.25630304217338562012f, -0.52031522989273071289f},
  {1.2611619234085083008f, 0.16350956261157989502f, 1.4819347858428955078f, -1.4511899948120117188f},
  {0.61175727844238281250f, 0.59606659412384033203f, -0.43141055107116699219f, 0.28225716948509216309f},
  {0.93741190433502197266f, -0.91375130414962768555f, -0.72180646657943725586f, -0.56444579362869262695f}
};
static const float tensor__2_Constant_output_0[4][4] = 
{
  {0.45717296004295349121f, -1.0414818525314331055f, -0.41980251669883728027f, 0.087266936898231506348f},
  {-0.13237179815769195557f, -0.024297239258885383606f, -0.43122747540473937988f, 0.027775403112173080444f},
  {0.26382121443748474121f, -0.069508567452430725098f, -0.22365584969520568848f, -0.28151130676269531250f},
  {0.20985217392444610596f, 0.10656242817640304565f, -0.24166084825992584229f, 0.80591464042663574219f}
};
static const float tensor__2_Constant_1_output_0[4] = 
{0.30824705958366394043f, 0.18561245501041412354f, 0.86405575275421142578f, -0.38421013951301574707f};
static const float tensor__2_Constant_2_output_0[4][4] = 
{
  {0.38273012638092041016f, -0.87189424037933349609f, -0.35144481062889099121f, 0.073057003319263458252f},
  {-0.49784576892852783203f, -0.091381080448627471924f, -1.6218316555023193359f, 0.10446233302354812622f},
  {1.4363969564437866211f, -0.37844529747962951660f, -1.2177132368087768555f, -1.5327122211456298828f},
  {0.25958344340324401855f, 0.13181583583354949951f, -0.29893019795417785645f, 0.99690216779708862305f}
};
static const float tensor__3_Constant_output_0[1][4] = 
{
  {0.56478399038314819336f, 0.35770490765571594238f, 0.53631824254989624023f, 0.51519805192947387695f}
};
static const float tensor__3_Constant_1_output_0[1] = 
{-0.24648539721965789795f};
static const int64_t tensor__0_Reshape_1_output_0[4] = 
{0, 0, 0, 1};
union tensor_union_0 {
float tensor__0_Pad_output_0[1][4];
float tensor__2_Gemm_output_0[1][4];
float tensor__2_MatMul_output_0[1][4];
};
static union tensor_union_0 tu0;

union tensor_union_1 {
float tensor__1_Gemm_output_0[1][4];
float tensor__1_MatMul_output_0[1][4];
float tensor__2_activation_Relu_output_0[1][4];
float tensor__2_Sub_output_0[1][4];
};
static union tensor_union_1 tu1;

union tensor_union_2 {
float tensor__1_activation_Relu_output_0[1][4];
float tensor__1_Sub_output_0[1][4];
};
static union tensor_union_2 tu2;


static inline void node__0_Pad( const float data[1][3], const int64_t pads[4], const float constant_value[1], float output[1][4] )
{
	/* Pad: 
	 * pad at start: 0 0 
	 * pad at end:   0 1 
	 * mode: constant
	 */
	uint32_t ir0;
	for( uint32_t o0=0, il0=0; o0<1; o0++ ) {
		bool pad_at_0=false;
		if( o0 < 0){
			pad_at_0= true;
		}
		else if( o0 < 1){
			ir0=il0;
			il0++;
		}
		else {
			pad_at_0= true;
		}
		uint32_t ir1;
		for( uint32_t o1=0, il1=0; o1<4; o1++ ) {
			bool pad_at_1=false;
			if( o1 < 0){
				pad_at_1= true;
			}
			else if( o1 < 3){
				ir1=il1;
				il1++;
			}
			else {
				pad_at_1= true;
			}
	if ( pad_at_0  || pad_at_1)
		output[o0][o1] = 0.0000000000000000000;
	else
		output[o0][o1]= data[ir0][ir1];
		}
	}
}

static inline void node__1_Gemm( const float tensor__0_Pad_output_0[1][4], const float tensor__1_Constant_output_0[4][4], const float tensor__1_Constant_1_output_0[4], float tensor__1_Gemm_output_0[1][4] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 4;
	const int N = 4;
	float (*A)[4]  = (float(*)[4])tensor__0_Pad_output_0;
	float (*Y)[4]  = (float(*)[4])tensor__1_Gemm_output_0;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[4]  = (float(*)[4])tensor__1_Constant_1_output_0;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor__1_Constant_output_0[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}

static inline void node__1_activation_Relu( const float tensor__1_Gemm_output_0[1][4], float tensor__1_activation_Relu_output_0[1][4] )
{
	/*Relu*/
	float *X = (float*)tensor__1_Gemm_output_0;
	float *Y = (float*)tensor__1_activation_Relu_output_0;
	for( uint32_t i=0; i<4; i++ )
		Y[i] = X[i] > 0 ? X[i] : 0;

}

static inline void node__1_MatMul( const float A[1][4], const float B[4][4], float Y[1][4] )
{
	/* MatMul */
	for( uint32_t r=0; r<1; r++ )
		for( uint32_t c=0; c<4; c++ ) {
			Y[r][c] = 0;
			for( uint32_t i=0; i<4; i++ )
				Y[r][c] += A[r][i] * B[i][c];
		}
}

static inline void node__1_Sub( const float tensor__0_Pad_output_0[1][4], const float tensor__1_MatMul_output_0[1][4], float tensor__1_Sub_output_0[1][4] )
{
	/* Sub
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<1; i0++) {
	for (unsigned i1=0; i1<4; i1++) {
		tensor__1_Sub_output_0[i0][i1] = tensor__0_Pad_output_0[0][i1]-tensor__1_MatMul_output_0[0][i1];;
	}
	}
}

static inline void node__2_Gemm( const float tensor__1_Sub_output_0[1][4], const float tensor__2_Constant_output_0[4][4], const float tensor__2_Constant_1_output_0[4], float tensor__2_Gemm_output_0[1][4] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 4;
	const int N = 4;
	float (*A)[4]  = (float(*)[4])tensor__1_Sub_output_0;
	float (*Y)[4]  = (float(*)[4])tensor__2_Gemm_output_0;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[4]  = (float(*)[4])tensor__2_Constant_1_output_0;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor__2_Constant_output_0[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][c] * beta;
			Y[r][c] = tmp;
	}
}

static inline void node__2_activation_Relu( const float tensor__2_Gemm_output_0[1][4], float tensor__2_activation_Relu_output_0[1][4] )
{
	/*Relu*/
	float *X = (float*)tensor__2_Gemm_output_0;
	float *Y = (float*)tensor__2_activation_Relu_output_0;
	for( uint32_t i=0; i<4; i++ )
		Y[i] = X[i] > 0 ? X[i] : 0;

}

static inline void node__2_MatMul( const float A[1][4], const float B[4][4], float Y[1][4] )
{
	/* MatMul */
	for( uint32_t r=0; r<1; r++ )
		for( uint32_t c=0; c<4; c++ ) {
			Y[r][c] = 0;
			for( uint32_t i=0; i<4; i++ )
				Y[r][c] += A[r][i] * B[i][c];
		}
}

static inline void node__2_Sub( const float tensor__1_Sub_output_0[1][4], const float tensor__2_MatMul_output_0[1][4], float tensor__2_Sub_output_0[1][4] )
{
	/* Sub
	   Implemented with Elementwise_2 template.
	   Attributes (these are the union of attributes for all 2-element-wise
	               operands. So most likely these values are ignored by onnx2c).
	   shift_dir: NOT_GIVEN
	   fmod: 0
	 */
	for (unsigned i0=0; i0<1; i0++) {
	for (unsigned i1=0; i1<4; i1++) {
		tensor__2_Sub_output_0[i0][i1] = tensor__1_Sub_output_0[0][i1]-tensor__2_MatMul_output_0[0][i1];;
	}
	}
}

static inline void node__3_Gemm( const float tensor__2_Sub_output_0[1][4], const float tensor__3_Constant_output_0[1][4], const float tensor__3_Constant_1_output_0[1], float tensor_43[1][1] )
{
	/* Gemm */
	/* alpha   = 1.0000000000000000000
	   beta    = 1.0000000000000000000
	   transA  = 0
	   transB  = 1
	 */
	const int M = 1;
	const int K = 4;
	const int N = 1;
	float (*A)[4]  = (float(*)[4])tensor__2_Sub_output_0;
	float (*Y)[1]  = (float(*)[1])tensor_43;
	float alpha = 1.0000000000000000000;
	float beta = 1.0000000000000000000;
	float (*C)[1]  = (float(*)[1])tensor__3_Constant_1_output_0;
	for( uint32_t r=0; r<M; r++ )
		for( uint32_t c=0; c<N; c++ ) {
			float ABrc = 0;
			for( uint32_t i=0; i<K; i++ ) {
				float B = tensor__3_Constant_output_0[c][i];
				ABrc += A[r][i] * B;
			}
			float tmp = ABrc * alpha;
			tmp += C[0][0] * beta;
			Y[r][c] = tmp;
	}
}


void entry(const float tensor_onnx__Pad_0[1][3], float tensor_43[1][1]) {
	node__0_Pad( tensor_onnx__Pad_0, tensor__0_Reshape_1_output_0, tensor__0_Constant_8_output_0, tu0.tensor__0_Pad_output_0);
	node__1_Gemm( tu0.tensor__0_Pad_output_0, tensor__1_Constant_output_0, tensor__1_Constant_1_output_0, tu1.tensor__1_Gemm_output_0);
	node__1_activation_Relu( tu1.tensor__1_Gemm_output_0, tu2.tensor__1_activation_Relu_output_0);
	node__1_MatMul( tu2.tensor__1_activation_Relu_output_0, tensor__1_Constant_2_output_0, tu1.tensor__1_MatMul_output_0);
	node__1_Sub( tu0.tensor__0_Pad_output_0, tu1.tensor__1_MatMul_output_0, tu2.tensor__1_Sub_output_0);
	node__2_Gemm( tu2.tensor__1_Sub_output_0, tensor__2_Constant_output_0, tensor__2_Constant_1_output_0, tu0.tensor__2_Gemm_output_0);
	node__2_activation_Relu( tu0.tensor__2_Gemm_output_0, tu1.tensor__2_activation_Relu_output_0);
	node__2_MatMul( tu1.tensor__2_activation_Relu_output_0, tensor__2_Constant_2_output_0, tu0.tensor__2_MatMul_output_0);
	node__2_Sub( tu2.tensor__1_Sub_output_0, tu0.tensor__2_MatMul_output_0, tu1.tensor__2_Sub_output_0);
	node__3_Gemm( tu1.tensor__2_Sub_output_0, tensor__3_Constant_output_0, tensor__3_Constant_1_output_0, tensor_43);
}
